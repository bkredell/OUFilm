# Statement on Artificial Intelligence 

We begin from a basic premise: before anything else, the university is a community of ideas. You bring yours, I bring mine, we share them, and we hope to learn from one another. Framed in this way, the problem of artificial intelligence is immediately apparent: if I relied on ChatGPT to write my lectures, or you relied on ChatGPT to write your assignments, then we are no longer sharing *our* ideas; instead, we are playacting at the parts of “professor” and “student,” while letting AI write our lines of dialogue for us.

Having said that, we also must recognize that our ways of knowing are changing around us. Just as the advent of the World Wide Web transformed our relationship to knowledge a generation ago, generative AI models are in the midst of doing the same today. Thus - at least in my opinion - trying to prevent my students from using AI would be to whistle past the graveyard.

In times of dramatic change, it’s worth bearing in mind durable principles. You came here to learn, not to recite. In that way, handing in an assignment that was “written” by generative AI is no different than copying an essay off the internet or recycling a term paper from a frat house filing cabinet - they are all about misrepresenting ideas that *aren’t* your own *as* your own. And that is, in a word, cheating. So don’t do that.

But learning to use technology responsibly is a necessity -  not only for the successful completion of this course, but for navigating the world that comes beyond.  You may find generative AI tools to be a useful research aid - if so, I encourage you to explore their use. But it’s important to not lose sight of the fundamental distinction between creation and recitation that I began with - we’re here to share ideas, not to deceive one another. 

Moreover, it's important to recognize that - at least for the large language model-based versions of generative AI that are currently dominating our discussion - these tools don't *know* anything. Rather, they are predictive inference-based tools; they can *sound* authoritative without actually saying anything authoritative at all. Philosophers have a particularly pungent way of describing work that meets this description; as Townsen Hicks, et al. note in a [recent paper](https://link.springer.com/article/10.1007/s10676-024-09775-5): "Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that looks truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit."

Undoubtedly, we will need to adjust our practice as students and as educators over the coming years as AI becomes more and more a part of how we learn. For now, I return to the above point about first principles: from a very early age in school, you should have learned the importance of showing your work. Whether it was your fourth-grade math teacher asking how you arrived at an answer, or your college film professor asking to see your research footnotes, we show our work so that there's evidence of our process, so that someone else can follow the trail of breadcrumbs we've left behind as we write. In this class, if you do use AI tools in the classroom, I'll be asking you to provide evidence (screenshots of chat transcripts, e.g.) of your use.

### Recommended Reading

Michael Townsen Hicks, James Humphries, Joe Slater, [ChatGPT is Bullshit](https://link.springer.com/article/10.1007/s10676-024-09775-5)

Noam Chomsky, Ian Roberts, Jeffrey Watamull, [The False Promise of ChatGPT](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)


